{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e92b9c64",
   "metadata": {},
   "source": [
    "# [Semantic Segmentation with Deep Learning](https://www.cc.gatech.edu/~hays/compvision/proj6/)\n",
    "\n",
    "For this project we are going to focus on semantic segmentation for 11 semantic categories with a state-of-the-art approach: deep learning.\n",
    "\n",
    "Basic learning objectives of this project:\n",
    "\n",
    "1. Understanding the ResNet architecture.\n",
    "2. Understand the concepts behind data augmentation and learning rate schedules for semantic segmentation\n",
    "3. Understand the role of dilated convolution and context in increasing the receptive field of a network.\n",
    "4. Experiment with different aspects of the training process and observe the performance.\n",
    "\n",
    "The starter code is mostly initialized to 'placeholder' just so that the starter code does not crash when run unmodified and you can get a preview of how results are presented.\n",
    "\n",
    "Your trained model should be able to produce an output like the one shown on the right below:\n",
    "\n",
    "Camvid Image | Model Prediction\n",
    ":-: | :--:\n",
    "<img src=\"https://user-images.githubusercontent.com/16724970/114431741-d6b7dd00-9b8d-11eb-8822-e7fa7e915e37.jpg\" width=\"300\"> | <img src=\"https://user-images.githubusercontent.com/16724970/114431739-d61f4680-9b8d-11eb-9266-e56aeb08476f.jpg\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee21d0a",
   "metadata": {},
   "source": [
    "## PSPNet and ResNet-50\n",
    "\n",
    "We'll be implementing PSPNet for this project, which uses a ResNet-50 backbone. ResNet-50 has 50 convolutional layers, which is significantly deeper than your SimpleNet of Project 5. We give you the implementation in `src/vision/resnet.py`. \n",
    "\n",
    "The ResNet-50 is composed of 4 different sections (each called a \"layer\"), named `layer1`, `layer2`, `layer3`, `layer4`. Each layer is composed of a repeated number of blocks, and each such block is named a `BottleNeck`. Specifically, `layer1` has 3 Bottlenecks, `layer2` has 4 Bottlenecks, `layer3` has 6 Bottlenecks, and `layer4` has 3 Bottlenecks. In all, ResNet-50 has 16 Bottlenecks, which accounts for 48 of the conv layers.\n",
    "\n",
    "### Visualizing a ResNet Bottleneck Module\n",
    "\n",
    "The BottleNeck has a residual connection, from which ResNet gets its name:\n",
    "\n",
    "<img width=\"300\" src=\"https://user-images.githubusercontent.com/16724970/114430171-2ac1c200-9b8c-11eb-8341-fc943ff0945f.png\">\n",
    "\n",
    "See Figure 5 of the [ResNet paper](https://arxiv.org/pdf/1512.03385.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb47f0d",
   "metadata": {},
   "source": [
    "### Implementing a Bottleneck\n",
    "\n",
    "The Bottleneck is implemented exactly as the figure above shows, with 1x1 Conv -> BN -> ReLU -> 3x3 Conv -> BN -> ReLU -> 1x1 Conv -> BN -> Optional Downsample -> Add Back Input -> ReLU. The channel dimension of the feature map will be expanded by 4x, as we can see by the conv layer `in_features` and `out_features` parameters. And notice that the stride is set at the `conv2` module, which will be very important later.\n",
    "\n",
    "```python\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "```\n",
    "\n",
    "and the forward method of the `Bottleneck` shows the residual connection. Notice that when we add back the input (the identity operation), we may need to downsample it for the shapes to match during the add operation (if the main branch downsampled the input):\n",
    "```python\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6814b13d",
   "metadata": {},
   "source": [
    "## Visualizing the Architecture\n",
    "Plotting the whole network architecture would require a massive figure, but we can show how data flows through just one Bottleneck, starting with 64 channels, and ending up with 256 output channels:\n",
    "<p float=\"left\">\n",
    "  <img src=\"https://user-images.githubusercontent.com/16724970/114427960-9eae9b00-9b89-11eb-9a3b-96817f205f32.png\" width=\"400\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f87188",
   "metadata": {},
   "source": [
    "## Part 1: Pyramid Pooling Module\n",
    "In Part 1, you will implement the Pyramid Pooling Module (PPM). After feeding an image through the ResNet backbone and obtaining a feature map, PSPNet aggregates context over different portions of the image with the PPM.\n",
    "\n",
    "The PPM splits the $H \\times W$ feature map into KxK grids. Here, 1x1, 2x2, 3x3,and 6x6 grids are formed, and features are average-pooled within each grid cell. Afterwards, the 1x1, 2x2, 3x3, and 6x6 grids are upsampled back to the original $H \\times W$ feature map resolution, and are stacked together along the channel dimension. These grids are visualized below (center):\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/16724970/114436422-4b414a80-9b93-11eb-8f02-8e7506b5f9a1.jpg\" width=\"900\">\n",
    "\n",
    "Implement this in `src/vision/part1_ppm.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8cec2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_PPM_6x6():  \u001b[32m\"Correct\"\u001b[0m\n",
      "test_PPM_fullres():  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from tests.test_part1_ppm import test_PPM_6x6, test_PPM_fullres\n",
    "from src.vision.utils import verify\n",
    "\n",
    "print(\"test_PPM_6x6(): \", verify(test_PPM_6x6))\n",
    "print(\"test_PPM_fullres(): \", verify(test_PPM_fullres))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6597f6",
   "metadata": {},
   "source": [
    "## Part 2: Dataset and Dataloader\n",
    "Next, in `src/vision/part2_dataset.py` you will implement the `make_dataset()` functions to create a list of paths to (image, ground truth) pairs. You will also implement the `__getitem__()` function that will load an RGB image and grayscale label map, and then apply a transform to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a4a9103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of (image,label) pairs train list generated!\n",
      "test_SemData_len():  \u001b[32m\"Correct\"\u001b[0m\n",
      "List of (image,label) pairs train list generated!\n",
      "test_getitem_no_data_aug():  \u001b[32m\"Correct\"\u001b[0m\n",
      "List of (image,label) pairs train list generated!\n",
      "test_make_dataset():  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from tests.test_part2_dataset import test_SemData_len, test_getitem_no_data_aug, test_make_dataset\n",
    "\n",
    "print(\"test_SemData_len(): \", verify(test_SemData_len))\n",
    "print(\"test_getitem_no_data_aug(): \", verify(test_getitem_no_data_aug))\n",
    "print(\"test_make_dataset(): \", verify(test_make_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3752c5",
   "metadata": {},
   "source": [
    "## Part 3: Online Data Preprocessing and Data Augmentation\n",
    "Data preprocessing and augmentation is very important to good performance, and we'll implement this in `src/vision/part3_training_utils.py`. We'll feed in square image crops to the network, but we must be careful to crop the same portion of the RGB image and ground truth semantic label map. Implement `get_train_transform(args)` and `get_val_transform(args)`, and check against the unit tests below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab36c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_get_train_transform():  \u001b[32m\"Correct\"\u001b[0m\n",
      "test_get_val_transform():  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from tests.test_part3_training_utils import test_get_train_transform, test_get_val_transform\n",
    "\n",
    "print(\"test_get_train_transform(): \", verify(test_get_train_transform))\n",
    "print(\"test_get_val_transform(): \", verify(test_get_val_transform))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ef17e",
   "metadata": {},
   "source": [
    "## Part 4: A Simple Segmentation Baseline\n",
    "We'll start with a very simple baseline -- a pretrained ResNet-50, without the final averagepool/fc layer, and a single 1x1 conv as a final classifier, converting the (2048,7,7) feature map to scores over 11 classes, a (11,7,7) tensor. Note that our output is just 7x7, which is very low resolution. Implement upsampling to the original height and width, and compute the loss and predicted class per pixel in `src/vision/part4_segmentation_net.py`.\n",
    "\n",
    "If the \"SimpleSegmentationNet\" architecture is specified in the experiment arguments (`args`), return this model in `get_model_and_optimizer()` in `part3_training_utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d5533da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_check_output_shapes():  \u001b[32m\"Correct\"\u001b[0m\n",
      "test_check_output_shapes_testtime():  \u001b[32m\"Correct\"\u001b[0m\n",
      "test_get_model_and_optimizer_simplearch():  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from tests.test_part4_segmentation_net import (\n",
    "    test_check_output_shapes,\n",
    "    test_check_output_shapes_testtime,\n",
    "    test_get_model_and_optimizer_simplearch\n",
    ")\n",
    "\n",
    "print(\"test_check_output_shapes(): \", verify(test_check_output_shapes))\n",
    "print(\"test_check_output_shapes_testtime(): \", verify(test_check_output_shapes_testtime))\n",
    "print(\"test_get_model_and_optimizer_simplearch(): \", verify(test_get_model_and_optimizer_simplearch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afa5cf7",
   "metadata": {},
   "source": [
    "## Part 5: Net Surgery for Increased Output Resolution and Receptive Field\n",
    "The basic ResNet-50 has two major problems:\n",
    "1. It does not have a large enough receptive field\n",
    "2. If run fully-convolutionally, it produces a low-resolution output (just $7 \\times 7$)!\n",
    "\n",
    "To fix the first problem, will need to replace some of its convolutional layers with dilated convolution. To fix the second problem, we'll reduce the stride of the network from 2 to 1, so that we don't downsample so much. Instead of going down to 7x7, we'll reduce to 28x28 for 224x224 input, or 26x26 for 201x201, like we do in this project. In other words, the downsampling rate will go from (1/32) to just (1/8).\n",
    "\n",
    "These animations depict how the dilated convolution (i.e. with dilation > 1) operation compares to convolution with no dilation (i.e. with dilation=1).\n",
    "\n",
    "Conv w/ Stride=1, Dilation=1 | Conv w/ Stride=2, Dilation=1 | Conv w/ Stride=1, Dilation=2\n",
    ":-: | :-: | :-:\n",
    "<img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif\" width=\"300\" align=\"center\"> | <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides.gif\" width=\"300\" align=\"center\"> | <img src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/dilation.gif\" width=\"300\" align=\"center\"> \n",
    "\n",
    "\n",
    "In Layer3, in every `Bottleneck`, we will change the 3x3 `conv2`, we will replace the conv layer that had stride=2, dilation=1, and padding=1 with a new conv layer, that instead  has stride=1, dilation=2, and padding=2. In the `downsample` block, we'll also need to hardcode the stride to 1, instead of 2.\n",
    "\n",
    "In Layer4, for every `Bottleneck`, we will make the same changes, except we'll change the dilation to 4 and padding to 4.\n",
    "\n",
    "Make these edits in `src/vision/part5_pspnet.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d6a97e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_pspnet_output_shapes(): \u001b[32m\"Correct\"\u001b[0m\n",
      "test_check_output_shapes_testtime_pspnet():  \u001b[32m\"Correct\"\u001b[0m\n",
      "test_check_output_shapes_zoom_factor_testtime_pspnet():  \u001b[32m\"Correct\"\u001b[0m\n",
      "test_get_model_and_optimizer_pspnet():  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from tests.test_part5_pspnet import (\n",
    "    test_pspnet_output_shapes,\n",
    "    test_check_output_shapes_testtime_pspnet,\n",
    "    test_get_model_and_optimizer_pspnet,\n",
    "    test_pspnet_output_with_zoom_factor\n",
    ")\n",
    "\n",
    "print(\"test_pspnet_output_shapes():\", verify(test_pspnet_output_shapes))\n",
    "print(\"test_check_output_shapes_testtime_pspnet(): \", verify(test_check_output_shapes_testtime_pspnet))\n",
    "print(\"test_check_output_shapes_zoom_factor_testtime_pspnet(): \", verify(test_pspnet_output_with_zoom_factor))\n",
    "\n",
    "print(\"test_get_model_and_optimizer_pspnet(): \", verify(test_get_model_and_optimizer_pspnet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7721b0c",
   "metadata": {},
   "source": [
    "## Part 6 Transfer Learning\n",
    "\n",
    "This section is required for CS 6476 students and optional for CS 4476.\n",
    "\n",
    "Use the model trained on Camvid as a pretrained model, and train it on Kitti Dataset. The Kitti dataloader is provided. Finish the model_and_optimizer function in part 6.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2461a35c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './initmodel/resnet50_v2.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtests\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtest_part6_kitti_dataset\u001b[39;00m \u001b[39mimport\u001b[39;00m test_model_kitti\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtest_model_kitti():\u001b[39m\u001b[39m\"\u001b[39m, verify(test_model_kitti))\n",
      "File \u001b[0;32m~/Github/project-5/src/vision/utils.py:51\u001b[0m, in \u001b[0;36mverify\u001b[0;34m(function)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m\"\"\"Will indicate with a print statement whether assertions passed or failed\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39mwithin function argument call.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m    string that is colored red or green when printed, indicating success\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     function()\n\u001b[1;32m     52\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m\\x1b\u001b[39;00m\u001b[39m[32m\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCorrect\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\x1b\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Github/project-5/tests/test_part6_kitti_dataset.py:66\u001b[0m, in \u001b[0;36mtest_model_kitti\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m psp_model \u001b[39m=\u001b[39m PSPNet(num_classes\u001b[39m=\u001b[39m\u001b[39m11\u001b[39m, pretrained\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m args \u001b[39m=\u001b[39m SimpleNamespace(\n\u001b[1;32m     54\u001b[0m \t\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\n\u001b[1;32m     55\u001b[0m \t\t\u001b[39m\"\u001b[39m\u001b[39mclasses\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m11\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \t}\n\u001b[1;32m     65\u001b[0m )\n\u001b[0;32m---> 66\u001b[0m model, _ \u001b[39m=\u001b[39m model_and_optimizer(args, psp_model)\n\u001b[1;32m     68\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m     69\u001b[0m H \u001b[39m=\u001b[39m \u001b[39m201\u001b[39m\n",
      "File \u001b[0;32m~/Github/project-5/src/vision/part6_transfer_learning.py:85\u001b[0m, in \u001b[0;36mmodel_and_optimizer\u001b[0;34m(args, model)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39mThis function is similar to get_model_and_optimizer in Part 3.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39m###########################################################################\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39m# TODO: YOUR CODE HERE                                                    #\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39m###########################################################################\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m model \u001b[39m=\u001b[39m PSPNet(\n\u001b[1;32m     86\u001b[0m     pretrained\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     87\u001b[0m     num_classes\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     88\u001b[0m     zoom_factor\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mzoom_factor\n\u001b[1;32m     89\u001b[0m )\n\u001b[1;32m     91\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD([\n\u001b[1;32m     92\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: model\u001b[39m.\u001b[39mlayer0\u001b[39m.\u001b[39mparameters(), \u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m: args\u001b[39m.\u001b[39mbase_lr, \u001b[39m'\u001b[39m\u001b[39mmomentum\u001b[39m\u001b[39m'\u001b[39m: args\u001b[39m.\u001b[39mmomentum, \u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m: args\u001b[39m.\u001b[39mweight_decay}, \n\u001b[1;32m     93\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: model\u001b[39m.\u001b[39mlayer1\u001b[39m.\u001b[39mparameters(), \u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m: args\u001b[39m.\u001b[39mbase_lr, \u001b[39m'\u001b[39m\u001b[39mmomentum\u001b[39m\u001b[39m'\u001b[39m: args\u001b[39m.\u001b[39mmomentum, \u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m: args\u001b[39m.\u001b[39mweight_decay}, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: model\u001b[39m.\u001b[39maux\u001b[39m.\u001b[39mparameters(), \u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m: args\u001b[39m.\u001b[39mbase_lr\u001b[39m*\u001b[39m\u001b[39m10.0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmomentum\u001b[39m\u001b[39m'\u001b[39m: args\u001b[39m.\u001b[39mmomentum, \u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m: args\u001b[39m.\u001b[39mweight_decay}\n\u001b[1;32m    100\u001b[0m ])        \n\u001b[1;32m    102\u001b[0m \u001b[39m###########################################################################\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39m#                             END OF YOUR CODE                            #\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m###########################################################################\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/project-5/src/vision/part5_pspnet.py:68\u001b[0m, in \u001b[0;36mPSPNet.__init__\u001b[0;34m(self, layers, bins, dropout, num_classes, zoom_factor, use_ppm, criterion, pretrained, deep_base)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maux \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m#############################################################################\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39m# TODO: YOUR CODE HERE                                                      #\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m# Initialize your ResNet backbone, and set the layers                       #\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39m# layer0, layer1, layer2, layer3, layer4. Note: layer0 should be sequential #\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m#############################################################################\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m resnet \u001b[39m=\u001b[39m resnet50(pretrained \u001b[39m=\u001b[39;49m pretrained, deep_base \u001b[39m=\u001b[39;49m deep_base)\n\u001b[1;32m     69\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresnet \u001b[39m=\u001b[39m resnet        \n\u001b[1;32m     70\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer0 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m     71\u001b[0m     resnet\u001b[39m.\u001b[39mconv1,\n\u001b[1;32m     72\u001b[0m     resnet\u001b[39m.\u001b[39mbn1,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m     resnet\u001b[39m.\u001b[39mmaxpool,\n\u001b[1;32m     81\u001b[0m )\n",
      "File \u001b[0;32m~/Github/project-5/src/vision/resnet.py:205\u001b[0m, in \u001b[0;36mresnet50\u001b[0;34m(pretrained, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mif\u001b[39;00m pretrained:\n\u001b[1;32m    203\u001b[0m     \u001b[39m# model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./initmodel/resnet50_v2.pth\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 205\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(model_path), strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    206\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv_proj5/lib/python3.10/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv_proj5/lib/python3.10/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv_proj5/lib/python3.10/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './initmodel/resnet50_v2.pth'"
     ]
    }
   ],
   "source": [
    "from tests.test_part6_kitti_dataset import test_model_kitti\n",
    "print(\"test_model_kitti():\", verify(test_model_kitti))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('cv_proj5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f0ce7c1787beef126cddd0c8e8f8dec6b85d0d5911c425f2ff821fc04d64cfca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
